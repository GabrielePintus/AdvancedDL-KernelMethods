\section{Exercise 2}

In this exercise, we will train a deep residual network on examples
generated by two specific functions. The first function is the 
6-dimensional multivariate complete Bell polynomial $B_6$, while
the second is the scrambled version of the first function $\tilde{B}_6$, as
described in the assignment.
\begin{align*}
    B_6(x_1, x_2, x_3, x_4, x_5, x_6) &= x_1^6 + 15x_2x_1^4 + 20x_3x_1^3 + 45x_2^2x_1^2 + 15x_2^3 \\
                                      &+ 60x_3x_2x_1 + 15x_4x_1^2 + 10x_3^2 + 15x_4x_2 \\
                                      &+ 6x_5x_1 + x_6 \\
\end{align*}


\subsection{Setup}

We begin by generating both the training and testing data for the two functions
by sampling from a uniform distribution and pass it through the functions.
\begin{align*}
    D_{train} &= \{(x_i, B_6(x_i))\}_{i=1}^{N}
    \\
    D_{test} &= \{(x_i, \tilde{B}_6(x_i))\}_{i=1}^{M}
    \\
    x_i &\sim \mathcal{U}(0, 1)^6
    \\
    N &= 10^5 \\
    M &= 6 \cdot 10^4
\end{align*}


\subsection{Training}

We use a nine-layers fully connected residual neural network and we train
it for 30 epochs with a batch size of 20. As usual, we use the Adam optimizer
and the mean squared error loss function. The learning rate is set to $10^{-3}$.
Hereafter we report the loss evolution plots for the training and testing data.

\input{content/plots/bell_loss.tex}
\begin{table}[H]
    \centering
    \begin{tabular}{c|cc}
        \toprule
        \textbf{Function} & Train Loss & Test Loss \\
        \midrule
        $B_6$ & \textbf{11.97} & \textbf{15.87} \\
        $\tilde{B}_6$ & 21.02 & 39.59 \\
        \bottomrule
    \end{tabular}
\end{table}

As we can see from the plots and the table, the network is able to learn
more easily the first function $B_6$ than the second $\tilde{B}_6$.
This provides empirical evidence in favor of the hypothesis that the
neural networks better approximate hierarchical compositional
functions. \\

If we then fix five of the six input variables, make the free variable
vary in the range $[0, 2]$ and plot the loss function, we obtain the following
plot.
\input{content/plots/bell_perturb.tex}

Which shows that the network is able to generalize well to perturbations
of the input data when considering the function $B_6$, but not when
considering the function $\tilde{B}_6$.